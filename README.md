![torchsmith](assets/torchsmith_logo_small_350px.jpg)

![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Test Coverage](.badges/coverage-badge.svg)
![torch](https://img.shields.io/badge/Python-3.10_|_3.11_|_3.12_|_3.13-blue.svg)
![torch](https://img.shields.io/badge/Pytorch-orange.svg)
### Understanding Generative AI By Building It From Scratch

---

## üî• Why Torchsmith?
Torchsmith is a minimalist library that focuses on understanding by building.
Torchsmith builds multimodal modern generative AI, such as **VAEs**, **VQVAEs**,
**autoregressive**,  **diffusion models** and **flow models** trained on image, text, and
image-text pairs.

Torchsmith is built using basic [PyTorch](https://pytorch.org/) operations,
without relying on high-level abstractions.

Here you will find a bare-bones implementation of various building blocks of
modern-day machine learning such as [attention](https://arxiv.org/abs/1706.03762),
positional encoding, transformers,
learning rate schedulers, various text and image tokenizers, to name a few.

## üé¨ Torchsmith In Action

Here is a quick highlight reel, for more experiments see the [experiments](#experiments).

### Image Generation

Image-space [diffusion](https://arxiv.org/abs/2006.11239) with [U-Net](https://arxiv.org/abs/1505.04597) and
[latent diffusion](https://arxiv.org/abs/2112.10752) with [Diffusion Transformer (DiT)](https://arxiv.org/abs/2212.09748)
on the [CIFAR 10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.

<p align="center">
    <img src="assets/cifar_10_original_data_c.png" alt="Original CIFAR 10 images"
    width="32%">
    <img src="assets/cifar_10_unet/sample_unet_diffusion_step_512.png"
    alt="Unconditional samples from UNet via image diffusion" width="32%">
    <img src="assets/cifar_10_DiT/cifar_10_class_conditioned_5.0.png" alt="Class
    conditioned samples from Diffusion Transformer via image diffusion" width="32%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. (Left) Original images from CIFAR 10 dataset.
    (Center) Unconditional samples generated by UNet using image-space diffusion.
    (Right) Class conditional samples generated by DiT using latent diffusion.
</p>

### Image-Text Generation

[GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)-style image-text generation decoder trained on the [Colored MNIST dataset](https://github.com/rll).
The model can be used to generate:
1. Unconditional image-text pairs
2. Text-conditioned images
3. Image-conditioned texts


<p align="center">
    <img src="assets/colored_mnist_with_text/image_completion_with_text_generation.png"
    alt="Image completion followed by text generation" width="900px"
    style="max-width: 100%;">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. For each image in the left-most column, only the non-darkened pixels are
    given as input to the GPT2 image-text model. To generate a sample,
    the model performs image completion followed by text captioning the contents of
    the completed image. Each row then shows the 4 samples i.e. 4 image-text pairs
    generated by the model.
</p>


### Text Generation

GPT2-style text generation decoder trained on [Eminem lyrics](https://huggingface.co/datasets/huggingartists/eminem):

```text
Sample #1
What, Im gonna get stay back to exam to stay
My dogs are crazy after nice playin to death
You know that wasnt spakin four
Thats my daughter, you wanna get score
Play if youre that videos
Oh, whoa, cause I wanna get so f***** circus
And someone and

Sample #2
And its feelin youse right!
Wouldnt f*** you to your kill: But you talk about me
Its about to have you still smack me
You talk to you think thats the shit you aint got some brains thats even you
When you hear on the floor, Im hardly startin to kill you!
```



### Representation Learning with VAEs

Variational Autoencoder ([VAE](https://arxiv.org/abs/1312.6114)) with convolutional
encoder and decoder to learn rich latent representations on Colored MNIST and
[SVHN](http://ufldl.stanford.edu/housenumbers/) datasets.

<p align="center">
    <img src="assets/colored_mnist_vae/interpolations.png" alt="Colored MNIST
    samples generated after linearly interpolating between a starting and
    endpoint"
    width="49%">
    <img src="assets/svhn_vae/interpolations.png" alt="SVHN
    samples generated after linearly interpolating between a starting and
    endpoint"
    width="49%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Walking the VAE latent space.
    Samples generated after linearly interpolating between a starting and
    endpoint in the latent space.
    Each row represents walking in the latent space with the left-most column as the
    starting point and the right-most column as the endpoint.
    (left) Colored MNIST dataset (right) SVHN dataset.
</p>

### Discrete Representation Learning with VQVAEs With Autoregressive Prior

Vector Quantized-Variational Autoencoders ([VQVAEs](https://arxiv.org/abs/1711.00937))
are trained from scratch to learn
discrete latent representations of images. An autoregressive transformer is
then trained on the learned discrete latent representations as a prior on top of the
VQVAE. This is then used to sample from the latent space and decode to image space
(using the VQVAE's decoder).

This can be done end-to-end as a 2-step process:
- **Step 1**: learning of the discrete latent representation via the VQVAE.
- **Step 2**: This is then followed by autoregressive modeling of the prior using a GPT2 on
top of the VQVAE's learned latent representation.

<p align="center">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step1_vqvae/epoch_1.png"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 1"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step1_vqvae/epoch_10.png"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 10"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step1_vqvae/epoch_20.png"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 20"
    width="32%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. SVHN Dataset. Step 1: VQVAE.
    50 (original image, reconstructed) pairs at the end of epoch 1 (left), epoch 10
    (center) and epoch 20 (right) (out of total 20 epochs).
    The reconstructed image is obtained by passing the original image through the
    VQVAE (encoder followed by decoder).
    One can notice how the reconstructed image gets closer to the original image as
    the training progresses.
</p>

<p align="center">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step2_gpt2/epoch_1.png"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 1"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step2_gpt2/epoch_10.png"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 10"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step2_gpt2/epoch_20.png"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 20"
    width="32%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. SVHN Dataset. Step 2: Autoregressive modeling of the prior using GPT2.
    100 unconditional samples generated at the end of epoch 1 (left), epoch 10
    (center) and epoch 20 (right) (out of total 20 epochs).
    Sampling is performed by the autoregressive model in latent space. The VQVAE
    from step 1 is then used to decode the latent space and decode to image space.
    One can notice the improvement in the quality of the generated image as the
    training progresses.
</p>

### [WIP] Flow Matching üöß
Transform noise (i.e. Gaussian distribution) to a target distribution by
simulating flows with ODEs and SDEs.
<p align="center">
    <img src="assets/flows/trajectories_mixture_of_gaussians.gif"
    alt="SVHN pairs: (original, reconstructed) pairs after epoch 1"
    width="100%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Simulation of an SDE (Langevin dynamics) that transforms noise (a Gaussian
    distribution) to a target distribution (here, a mixture of Gaussians) over time.
    One can see how the points start out normally distributed and over time are
    transformed to move towards the target distribution (blue shaded region). The
    Langevin dynamics involve both a drift and diffusion coefficient.
    Blue shaded region: Target distribution (mixture of Gaussians) (seen on both).
    Black 'x' markers: Transformed samples at time t (seen on the left).
    Contour lines: Density estimation of the transformed distribution at time t
(seen on the right).
</p>



---

## ‚ú® Features

- üß† **Modality Agnostic Autoregression Trainer**

  A modular and minimal model trainer that works seamlessly across:
  - Text generation
  - Image generation
  - Multi-modal tasks: Text-to-Image and Image-to-Text


- üé® **Diffusion Trainer For Images**
  - Image Diffusion (diffusion in pixel space)
  - Latent Diffusion (diffusion in latent space e.g. using VQ-VAE as encoder)


- üîÅ **VAE and VQVAEs For Images**
  - Convolutional Variational Autoencoder to learn a meaningful and compact latent
    space. The VAE can also be used to generate new samples.
  - VQVAEs learn discrete latent representations allowing modeling of the prior
    using an autoregressive model which is well-suited (for generating in) the discrete
    representation space.
  - End-to-end VQVAE with autoregressive prior trained on the learned discrete latent representations.


- ü§ñ **Implementation Of Modern Generative Architectures From The Ground Up**
  - Modality agnostic GPT2
  - UNet for images
  - DiT (Diffusion Transformer) for images


- üßæ **Tokenizers For Text**
  - Character-level tokenizer
  - Word-level tokenizer
  - Byte Pair Encoding (BPE)
    - Efficiently implemented using Python iterators and joblib


- üõ†Ô∏è **Primitive Operations**
  - Core components like attention, MLPs, positional encodings, and layer norms are
    implemented from first principles
  - No reliance on `torch.nn.Transformer` or other high-level attention
    blocks


- üß™ **Test-Driven Development**
  - Full suite of unit tests ensuring correctness and stability
  - Easy to extend and experiment without breaking things


- üìã **Best Practices For Code Development**
  - Uses [`uv`](https://github.com/astral-sh/uv) for dependency management
  - Enforces style and formatting with [`pre-commit`](https://pre-commit.com/) and
    [`ruff`](https://github.com/astral-sh/ruff)

---

## Experiments

### [Autoregression] Colored MNIST With Text Labels
This experiment involves using GPT2-style decoder trained with autoregression.
In addition to the results shown in [this section](#image-text-generation), here
are detailed results:

<p align="center">
    <img src="assets/colored_mnist_with_text/unconditional_image_and_text_generation.png"
    alt="Unconditional image-text pair generation" width="900px"
    style="max-width: 100%;">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Unconditional image-text pair generation.
</p>


<p align="center">
    <img src="assets/colored_mnist_with_text/text_conditioned_digits.png"
  alt="Image generations conditioned on the given text" width="900px"
  style="max-width: 100%;">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Generated images conditioned on text "dark red {DIGIT} on light cyan" where
    DIGIT takes
    on values of the digit names from 1-9 (e.g. "dark red one on light cyan", etc.).
    The model conditions the generated image on the text.
</p>


<p align="center">
    <img src="assets/colored_mnist_with_text/partial_text_conditioned_digits.png"
    alt="Text completion followed by image generation conditioned on the text"
    width="900px" style="max-width: 100%;">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Generated images conditioned on text "plain red {DIGIT}" where DIGIT takes
    on values of the digit names from 1-9 (e.g. "plain red one", "plain red two", etc.).
    Note that the model first completes the text to determine the background color
    ("plain red one" -> "plain red one on dark green") and
    then goes on to generate the corresponding image.
</p>


### [Diffusion] CIFAR 10

In addition to the results shown in [this section](#image-generation), here
are detailed results:

#### UNet Experiments

<p align="center">
    <img src="assets/cifar_10_unet/sample_unet_diffusion_step_4.png" alt="CIFAR 10
    generated images after 4 denoising steps"
    width="19%">
    <img src="assets/cifar_10_unet/sample_unet_diffusion_step_16.png" alt="CIFAR 10
    generated images after 16 denoising steps"
    width="19%">
    <img src="assets/cifar_10_unet/sample_unet_diffusion_step_64.png" alt="CIFAR 10
    generated images after 64 denoising steps"
    width="19%">
    <img src="assets/cifar_10_unet/sample_unet_diffusion_step_256.png" alt="CIFAR 10
    generated images after 256 denoising steps"
    width="19%">
    <img src="assets/cifar_10_unet/sample_unet_diffusion_step_512.png" alt="CIFAR 10
    generated images after 512 denoising steps"
    width="19%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. CIFAR 10 samples generated after 4, 16, 64, 256, and 512 denoising steps
    (from left to right). Notice
    how the details in the generated samples are proportional to the number of
    denoising steps.
</p>


<p align="center">
    <img src="assets/cifar_10_unet/training/epoch_1.png" alt="CIFAR 10
    unconditionally generated images after epoch 1"
    width="19%">
    <img src="assets/cifar_10_unet/training/epoch_5.png" alt="CIFAR 10
    unconditionally generated images after epoch 5"
    width="19%">
    <img src="assets/cifar_10_unet/training/epoch_15.png" alt="CIFAR 10
    unconditionally generated images after epoch 15"
    width="19%">
    <img src="assets/cifar_10_unet/training/epoch_30.png" alt="CIFAR 10
    unconditionally generated images after epoch 30"
    width="19%">
    <img src="assets/cifar_10_unet/training/epoch_60.png" alt="CIFAR 10
    unconditionally generated images after epoch 60"
    width="19%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Unconditional CIFAR 10 samples generated after epoch 1, 5, 15, 30 and 60
    (from left to right). Sampled images consist only of high-frequency noise as seen
    after
    epoch 1 but details begin to appear and images become coherent as the training
    progresses.
</p>

#### Diffusion Transformer (DiT) Experiments


<p align="center">
    <img src="assets/cifar_10_DiT/cifar_10_class_conditioned.jpg" alt="CIFAR 10
    generated class-conditioned images using no classifier free guidance "
    width="19%">
    <img src="assets/cifar_10_DiT/cifar_10_class_conditioned_1.0.png" alt="CIFAR 10
    generated class-conditioned images using classifier free guidance 1.0"
    width="19%">
    <img src="assets/cifar_10_DiT/cifar_10_class_conditioned_3.0.png" alt="CIFAR 10
    generated class-conditioned images using classifier free guidance 3.0"
    width="19%">
    <img src="assets/cifar_10_DiT/cifar_10_class_conditioned_5.0.png" alt="CIFAR 10
    generated class-conditioned images using classifier free guidance 5.0"
    width="19%">
    <img src="assets/cifar_10_DiT/cifar_10_class_conditioned_7.5.png" alt="CIFAR 10
    generated class-conditioned images using classifier free guidance 7.5"
    width="19%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Class-conditioned CIFAR 10 samples generated with no classifier free
    guidance (CFG),
    classifier free guidance with weight 1.0, 3.0, 5.0 and 7.5 (from left to right).
    Larger values of the CFG weight improve the faithfulness of the generated image
    to the class. With too large weights, the samples begin to lose diversity and
    collapse to similar looking samples.
</p>


<p align="center">
    <img src="assets/cifar_10_DiT/training/samples_at_1.png" alt="CIFAR 10
    unconditionally generated images after epoch 1"
    width="19%">
    <img src="assets/cifar_10_DiT/training/samples_at_3.png" alt="CIFAR 10
    unconditionally generated images after epoch 3"
    width="19%">
    <img src="assets/cifar_10_DiT/training/samples_at_5.png" alt="CIFAR 10
    unconditionally generated images after epoch 5"
    width="19%">
    <img src="assets/cifar_10_DiT/training/samples_at_30.png" alt="CIFAR 10
    unconditionally generated images after epoch 30"
    width="19%">
    <img src="assets/cifar_10_DiT/training/samples_at_60.png" alt="CIFAR 10
    unconditionally generated images after epoch 60"
    width="19%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. Unconditional CIFAR 10 samples generated after epoch 1, 3, 5, 30 and 60
    (from left to right). Sampled images become coherent and detailed as the training
    progresses.
</p>



<p align="center">
  <img src="assets/cifar_10_DiT/training/scheduler.png" alt="Learning rate scheduler" height="250">
  <img src="assets/cifar_10_DiT/training/training.png" alt="Training and Testing losses" height="250">
</p>
<p align="center" style="font-size: smaller; color: gray;">
  Fig. (left) Cosine learning rate scheduler with warmup. (right) Train and test losses.
</p>


### [Autoregression] Poetry

Here are some samples that are generated by GPT2 model trained on the [poetry dataset](https://huggingface.co/datasets/merve/poetry):
```text
Sample #1
Long Pystand Pelite: 191[Dweven
Pestate known that they recicish or latt.
Untrunter-vain-his and-lifes.

Sample #2
With lover blowdes and stake a the shade unto more,
The gively . This daight that I dispure.
Revaliants stempeted golding of t

Sample #3
Dramters my not be and mensuck'd withat might and of this,
Then with an shike of sturn
Of wift be my kimery arm thing fair dre
```


### [Autoencoders] VAE


#### Colored MNIST

The Colored MNIST dataset is modeled using a convolutional VAE with a
32-dimensional latent space.

<p align="center">
    <img src="assets/colored_mnist_vae/reconstructions.png" alt="Pairs of original
    image (from the dataset) and reconstructed image"
    width="33%">
    <img src="assets/colored_mnist_vae/interpolations.png" alt="Colored MNIST
    samples generated after linearly interpolating between a starting and
    endpoint"
    width="33%">
    <img src="assets/colored_mnist_vae/samples.png" alt="Colored MNIST
    samples after 20 epochs"
    width="33%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. (left) 50 pairs consisting of the original image (from the Colored MNIST
    dataset) and reconstructed image.
    Reconstruction is performed by first encoding and then decoding the original image.
    (center) Colored MNIST
    samples generated after linearly interpolating between a starting and
    endpoint in the latent space.
    Each row represents walking in the latent space with the left-most column as the
    starting point and the right-most column as the endpoint.
    (right) Colored MNIST
    samples generated by randomly sampling the latent space followed by
    decoding.
</p>

#### SVHN
The Street View House Numbers dataset is modeled using a convolutional VAE with a
16-dimensional latent space.

<p align="center">
    <img src="assets/svhn_vae/reconstructions.png" alt="Pairs of original
    image (from the dataset) and reconstructed image"
    width="32%">
    <img src="assets/svhn_vae/interpolations.png" alt="SVHN
    samples generated after linearly interpolating between a starting and
    endpoint"
    width="32%">
    <img src="assets/svhn_vae/samples.png" alt="SVHN
    samples after 10 epochs"
    width="32%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. (left) 50 pairs consisting of the original image (from the SVHN dataset) and
    reconstructed image.
    Reconstruction is performed by first encoding and then decoding the original image.
    (center) SVHN samples generated after linearly interpolating between a starting and
    endpoint in the latent space.
    Each row represents walking in the latent space with the left-most column as the
    starting point and the right-most column as the endpoint.
    (right) SVHN samples generated by randomly sampling the latent space followed by
    decoding.
</p>


### [Autoencoders] VQVAE
Read the preliminaries
[here](#discrete-representation-learning-with-vqvaes-with-autoregressive-prior).

#### SVHN
See the section for SVHN
[here](#discrete-representation-learning-with-vqvaes-with-autoregressive-prior).

<p align="center">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step1_vqvae/epoch_20.png" alt="SVHN
pairs: (original, reconstructed) pairs"
    width="49%">
    <img src="assets/vqvae_w_gpt2_prior/svhn/step2_gpt2/epoch_20.png"
    alt="Unconditional samples generated using the autoregressive prior" width="49%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. SVHN Dataset. (Left) Step 1: VQVAE. 50 (original image, reconstructed) pairs. The
reconstructed image is obtained by passing the original image through the VQVAE
(encoder followed by decoder).
    (Right) Step 2: Autoregressive GPT2 trained on VQVAE's learned latent
representation. 100 unconditional samples generated by the autoregressive model
which are then decoded from the learned discrete latent representations to the image space.
</p>

#### CIFAR-10


<p align="center">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step1_vqvae/epoch_1.png"
    alt="CIFAR-10 pairs: (original, reconstructed) pairs after epoch 1"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step1_vqvae/epoch_10.png"
    alt="CIFAR-10 pairs: (original, reconstructed) pairs after epoch 10"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step1_vqvae/epoch_20.png"
    alt="CIFAR-10 pairs: (original, reconstructed) pairs after epoch 20"
    width="32%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. CIFAR-10 Dataset. Step 1: VQVAE.
    50 (original image, reconstructed) pairs at the end of epoch 1 (left), epoch 10
    (center) and epoch 20 (right) (out of total 20 epochs).
    The reconstructed image is obtained by passing the original image through the
    VQVAE (encoder followed by decoder).
    One can notice how the reconstructed image gets closer to the original image as
    the training progresses.
</p>

<p align="center">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step2_gpt2/epoch_1.png"
    alt="CIFAR-10 pairs: (original, reconstructed) pairs after epoch 1"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step2_gpt2/epoch_10.png"
    alt="CIFAR-10 pairs: (original, reconstructed) pairs after epoch 10"
    width="32%">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step2_gpt2/epoch_20.png"
    alt="CIFAR-10 pairs: (original, reconstructed) pairs after epoch 20"
    width="32%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. CIFAR-10 Dataset. Step 2: Autoregressive modeling of the prior using GPT2.
    100 unconditional samples generated at the end of epoch 1 (left), epoch 10
    (center) and epoch 20 (right) (out of total 20 epochs).
    Sampling is performed by the autoregressive model in latent space. The VQVAE
    from step 1 is then used to decode the latent space and decode to image space.
    One can notice the improvement in the quality of the generated image as the
    training progresses.
</p>

<p align="center">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step1_vqvae/epoch_20.png" alt="CIFAR-10
pairs: (original, reconstructed) pairs"
    width="49%">
    <img src="assets/vqvae_w_gpt2_prior/cifar_10/step2_gpt2/epoch_20.png"
    alt="Unconditional samples generated using the autoregressive prior" width="49%">
</p>
<p align="center" style="font-size: smaller; color: gray;">
    Fig. CIFAR-10 Dataset. (Left) Step 1: VQVAE. 50 (original image, reconstructed)
pairs.
The
reconstructed image is obtained by passing the original image through the VQVAE
(encoder followed by decoder).
    (Right) Step 2: Autoregressive GPT2 trained on VQVAE's learned latent
representation. 100 unconditional samples generated by the autoregressive model
which are then decoded from the learned discrete latent representations to the image space.
</p>

---

## üì¶ Installation

```bash
# Clone the repo
git clone https://github.com/ankitdhall/torchsmith.git

# Option 1: Install using uv
uv pip install ./torchsmith

# Option 2: Install using pip
pip install ./torchsmith
```

---

## üß∞ Usage Examples

### üí™ Training
Training scripts for various datasets and models can be found in the `examples/`
directory.

For example:
```bash
python examples/train_eminem.py
```

### üé® Generating Samples

```python
import huggingface_hub

from torchsmith.models.gpt2.decoder import GPT2Decoder
from torchsmith.tokenizers.mnist_tokenizer import ColoredMNISTImageAndTextTokenizer
from torchsmith.tokenizers.mnist_tokenizer import (
    colored_mnist_with_text_conditioned_on_text,
)
from torchsmith.utils.plotting import plot_images
from torchsmith.utils.pytorch import get_device

text = "plain orange seven on dark blue"
num_samples = 4

tokenizer = ColoredMNISTImageAndTextTokenizer()
path_to_weights = huggingface_hub.hf_hub_download(
    "ankitdhall/colored_mnist_with_text_gpt2", filename="model.pth"
)
transformer = GPT2Decoder.load_model(path_to_weights).to(get_device())

decoded_images, decoded_texts = colored_mnist_with_text_conditioned_on_text(
    num_samples=num_samples,
    text=text,
    tokenizer=tokenizer,
    transformer=transformer,
)
plot_images(
    decoded_images, titles=decoded_texts, max_cols=int(num_samples ** 2)
)
```

---

## üß™ Test-Driven Development

Torchsmith strives for test-driven development and uses `pytest` for testing.
The tests cover most of the codebase.

First install packages needed for testing:
```bash
# Option 1: Install using uv
uv pip install ./torchsmith[testing]

# Option 2: Install using pip
pip install ./torchsmith[testing]
```

To run the tests:
```bash
pytest tests/
```

To generate and save (saved to `.badges/`) the test coverage badge run:
```bash
./scripts/generate_badges.sh
```

---

## üßë‚Äçüíª Contributing

When contributing changes, please:
- Add tests for new features, improvements and bug-fixes.
- Follow the existing coding style.

Torchsmith uses pre-commit hooks to ensure clean and consistent code:
```bash
pre-commit install
pre-commit run --all-files
```

---

## üöß TODOs

- [ ] Experiment with learning rate schedulers
- [ ] No-code way to train using declarative YAML config
- [x] Experiment with VAEs
- [x] Experiment with VQ-VAEs
- [ ] [WIP] Flow matching üöß
- [ ] Experiment with VQ-GANs
- [ ] Support LoRA and fine-tuning utilities
- [ ] Find bigger GPUs and extend to larger datasets
- [ ] Experiment with different positional embeddings
- [ ] Replace .reshape with .view in the codebase; see [ref.](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)


## Acknowledgements
Inspired by:
- Works such as [minGPT](https://github.com/karpathy/minGPT) and [nanoGPT](https://github.com/karpathy/nanoGPT).
- [Berkeley's CS294-158 Deep Unsupervised Learning](https://sites.google.com/view/berkeley-cs294-158-sp24/home). Also, thanks to them for the
  Colored MNIST dataset and the pre-trained VQ-VAEs.
- [MIT's 6.S184](https://diffusion.csail.mit.edu/) and
[Flow matching](https://arxiv.org/abs/2412.06264)
